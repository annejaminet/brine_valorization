{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import rasterio\n",
    "import requests\n",
    "import zipfile\n",
    "import io\n",
    "import os\n",
    "import shutil\n",
    "from shapely.geometry import Point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_data_from_url(url, zipped=False, filepath=None, **kwargs):\n",
    "    \"\"\"\n",
    "    Extracts and returns raw data from a URL.\n",
    "\n",
    "    Parameters:\n",
    "    url : str\n",
    "        The URL of the file to be extracted.\n",
    "    zipped : bool, optional\n",
    "        If True, the file is expected to be inside a zip file. Default is False.\n",
    "    filepath : str, optional\n",
    "        If zipped is True, this is the path to the file inside the archive.\n",
    "    **kwargs :\n",
    "        Additional arguments passed to the appropriate reader function \n",
    "        (e.g., pd.read_csv, pd.read_excel, gpd.read_file, rasterio.open)\n",
    "\n",
    "    Returns:\n",
    "    data : DataFrame, GeoDataFrame, or rasterio.DatasetReader\n",
    "    \"\"\"\n",
    "    def detect_file_type(path):\n",
    "        ext = os.path.splitext(path.lower())[1]\n",
    "        if ext in ['.csv']: return 'csv'\n",
    "        if ext in ['.txt']: return 'txt'\n",
    "        elif ext in ['.xls']: return 'xls'\n",
    "        elif ext in ['.xlsx']: return 'xlsx'\n",
    "        elif ext in ['.shp', '.geojson', '.gpkg', '.json', '.kml']: return 'vector'\n",
    "        elif ext in ['.tif', '.tiff']: return 'raster'\n",
    "        else: return 'unknown'\n",
    "\n",
    "    response = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "    if response.status_code != 200:\n",
    "        response.raise_for_status()\n",
    "\n",
    "    if not zipped:\n",
    "        file_type = detect_file_type(url)\n",
    "        buffer = io.BytesIO(response.content)\n",
    "\n",
    "        if file_type == 'csv' or file_type == 'txt':\n",
    "            data = pd.read_csv(buffer, low_memory=False, **kwargs)\n",
    "        elif file_type == 'xls':\n",
    "            data = pd.read_excel(buffer, engine='xlrd', **kwargs)\n",
    "        elif file_type == 'xlsx':\n",
    "            data = pd.read_excel(buffer, engine='openpyxl', **kwargs)\n",
    "        elif file_type == 'vector':\n",
    "            data = gpd.read_file(buffer, **kwargs).to_crs(saws_crs)\n",
    "        elif file_type == 'raster':\n",
    "            data = rasterio.open(buffer, **kwargs)\n",
    "        else: # fallback on csv\n",
    "            try:\n",
    "                data = pd.read_csv(buffer, low_memory=False, **kwargs) \n",
    "            except:\n",
    "                raise ValueError(f\"Could not download data from this URL. Please check URL and try again.\")\n",
    "\n",
    "    else:\n",
    "        with zipfile.ZipFile(io.BytesIO(response.content)) as zip_file:\n",
    "            if filepath is None:\n",
    "                raise ValueError(\"Must specify 'filepath' within ZIP archive.\")\n",
    "            file_type = detect_file_type(filepath)\n",
    "\n",
    "            try:\n",
    "                with zip_file.open(filepath) as file:\n",
    "                    if file_type == 'csv':\n",
    "                        data = pd.read_csv(file, low_memory=False, **kwargs)\n",
    "                    elif file_type == 'xls':\n",
    "                        data = pd.read_excel(file, engine='xlrd', **kwargs)\n",
    "                    elif file_type == 'xlsx':\n",
    "                        data = pd.read_excel(file, engine='openpyxl', **kwargs)\n",
    "                    elif file_type in ['vector', 'raster']:\n",
    "                        raise NotImplementedError(\"Shapefiles and rasters require extraction.\")\n",
    "            except:\n",
    "                zip_file.extractall('extracted_data')\n",
    "                full_path = os.path.abspath(os.path.join('extracted_data', filepath))\n",
    "                if file_type == 'csv':\n",
    "                    data = pd.read_csv(full_path, low_memory=False, **kwargs)\n",
    "                elif file_type == 'xls':\n",
    "                    data = pd.read_excel(full_path, engine='xlrd', **kwargs)\n",
    "                elif file_type == 'xlsx':\n",
    "                    data = pd.read_excel(full_path, engine='openpyxl', **kwargs)\n",
    "                elif file_type == 'vector':\n",
    "                    data = gpd.read_file(full_path, **kwargs).to_crs(saws_crs)\n",
    "                elif file_type == 'raster':\n",
    "                    data = rasterio.open(full_path, **kwargs)\n",
    "                shutil.rmtree('extracted_data')\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def lat_long_to_point(df, lat_col, long_col):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function takes a DataFrame with lat/long columns stored as floats\n",
    "    and converts it into a GeoDataFrame with a point geometry.\n",
    "\n",
    "    Arguments:\n",
    "    df : DataFrame\n",
    "        The DataFrame to be converted.\n",
    "    lat_col : str\n",
    "        The name of the column containing latitude values.\n",
    "    long_col : str\n",
    "        The name of the column containing longitude values.\n",
    "\n",
    "    Returns:\n",
    "    df : GeoDataFrame\n",
    "        The modified DataFrame as a GeoDataFrame with point geometry.\n",
    "    \"\"\"\n",
    "\n",
    "    df_geo = [Point(lon, lat) for lon, lat in zip(df[long_col], df[lat_col])]\n",
    "    df = gpd.GeoDataFrame(df, geometry=df_geo, crs = 'EPSG:4326')\n",
    "    df = df.to_crs(saws_crs)\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  sourceID  unique_site_ID samp_id  proprietary_data state_alpha fips_cd  \\\n",
      "0    AZDEQ  AZDEQ-az000779     NaN                 0          AZ   04021   \n",
      "1    AZDEQ  AZDEQ-az001146     NaN                 0          AZ   04013   \n",
      "2    AZDEQ  AZDEQ-az002056     NaN                 0          AZ   04021   \n",
      "3    AZDEQ  AZDEQ-az002153     NaN                 0          AZ   04021   \n",
      "4    AZDEQ  AZDEQ-az006219     NaN                 0          AZ   04013   \n",
      "\n",
      "  county_nm  dec_lat_va  dec_long_va location_flag  ...  \\\n",
      "0     Pinal   32.938241  -111.936083           2.0  ...   \n",
      "1  Maricopa   33.005930  -112.676581           2.0  ...   \n",
      "2     Pinal   32.676461  -111.516639           2.0  ...   \n",
      "3     Pinal   33.065584  -111.975132           2.0  ...   \n",
      "4  Maricopa   33.518722  -113.069351           2.0  ...   \n",
      "\n",
      "   model_pp_Anhydrite_closed_16x  model_pp_Barite_closed_16x  \\\n",
      "0                            0.0                         0.0   \n",
      "1                            0.0                         0.0   \n",
      "2                            0.0                         0.0   \n",
      "3                            0.0                         0.0   \n",
      "4                            0.0                         0.0   \n",
      "\n",
      "   model_pp_Calcite_closed_16x model_pp_Celestite_closed_16x  \\\n",
      "0                       0.0008                           0.0   \n",
      "1                       0.0006                           0.0   \n",
      "2                       0.0005                           0.0   \n",
      "3                       0.0015                           0.0   \n",
      "4                       0.0001                           0.0   \n",
      "\n",
      "   model_pp_Chalcedony_closed_16x  model_pp_Gypsum_closed_16x  \\\n",
      "0                             0.0                      0.0005   \n",
      "1                             0.0                      0.0000   \n",
      "2                             0.0                      0.0000   \n",
      "3                             0.0                      0.0004   \n",
      "4                             0.0                      0.0000   \n",
      "\n",
      "   model_pp_Halite_closed_16x  model_pp_Ferrihydrite_closed_16x  \\\n",
      "0                         0.0                               0.0   \n",
      "1                         0.0                               0.0   \n",
      "2                         0.0                               0.0   \n",
      "3                         0.0                               0.0   \n",
      "4                         0.0                               0.0   \n",
      "\n",
      "   model_pp_mgL_closed_16x  model_os_p_closed_16x  \n",
      "0                 158.6831                 5.5103  \n",
      "1                  61.6574                14.8014  \n",
      "2                  48.8269                 2.0916  \n",
      "3                 215.7561                15.0517  \n",
      "4                   7.4063                 5.2651  \n",
      "\n",
      "[5 rows x 376 columns]\n"
     ]
    }
   ],
   "source": [
    "data_url = 'https://www.sciencebase.gov/catalog/file/get/58937228e4b0fa1e59b73361?f=__disk__5a%2Fae%2F1a%2F5aae1aa25f84b94737628e43ef82e34f6897a63b'\n",
    "\n",
    "data=load_data_from_url(data_url, zipped=True, filepath='Major_Ions.csv')\n",
    "print(data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in original data: 123699\n",
      "Number of rows in filtered data (TDS >= 1000 mg/L): 23454\n",
      "Number of rows excluded (TDS < 1000 mg/L): 100245\n"
     ]
    }
   ],
   "source": [
    "# Only include samples where TDS_mgL is greater than or equal to 1000\n",
    "filtered_data = data[data['TDS_mgL'] >= 1000]\n",
    "\n",
    "num_rows_total = len(data)  # total rows before filtering\n",
    "num_rows_filtered = len(filtered_data)  # rows after filtering\n",
    "num_rows_excluded = num_rows_total - num_rows_filtered\n",
    "\n",
    "print(\"Number of rows in original data:\", num_rows_total)\n",
    "print(\"Number of rows in filtered data (TDS >= 1000 mg/L):\", num_rows_filtered)\n",
    "print(\"Number of rows excluded (TDS < 1000 mg/L):\", num_rows_excluded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV with column headers saved as column_headers.csv\n"
     ]
    }
   ],
   "source": [
    "# Create a DataFrame with just the column names as a single column\n",
    "pd.Series(data.columns, name=\"Column_Name\").to_csv(\"column_headers_list.csv\", index=False)\n",
    "\n",
    "print(\"CSV with column headers saved as column_headers.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in filtered data (model_pct_err.abs() <= 10): 16706\n",
      "Number of rows excluded (model_pct_err.abs() >= 10): 6748\n"
     ]
    }
   ],
   "source": [
    "filtered_data2 = filtered_data[filtered_data['model_pct_err'].abs() <= 10]\n",
    "\n",
    "num_rows_filtered2 = len(filtered_data2)  # rows after filtering\n",
    "num_rows_excluded2 = num_rows_filtered - num_rows_filtered2\n",
    "\n",
    "print(\"Number of rows in filtered data (model_pct_err.abs() <= 10):\", num_rows_filtered2)\n",
    "print(\"Number of rows excluded (model_pct_err.abs() >= 10):\", num_rows_excluded2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining rows after removing samples with missing key parameters: 14943\n",
      "Number of rows excluded due to missing key parameters: 1763\n"
     ]
    }
   ],
   "source": [
    "\n",
    "required_columns = [\n",
    "    'well_depth_ft',       # well depth\n",
    "    'Temp_C',    # temperature in Celsius -> alternative is model_temp_c\n",
    "    'ph',                # pH measurement -> alternative is model_pH\n",
    "    'Alk_mgL',   # alkalinity as mg/L CaCO3 -> alternative is model_Alk_eqL (or Alk_mgL_flag)\n",
    "    'Si_mgL'          # silica in mg/L --> alternative is model_Si_molL (or Si_mgL_flag)\n",
    "]\n",
    "\n",
    "# Drop rows where ANY of these columns are missing\n",
    "filtered_data3 = filtered_data2.dropna(subset=required_columns)\n",
    "\n",
    "num_rows_filtered3 = len(filtered_data3)  # rows after filtering\n",
    "num_rows_excluded3 = num_rows_filtered2 - num_rows_filtered3\n",
    "\n",
    "print(\"Remaining rows after removing samples with missing key parameters:\",\n",
    "      len(filtered_data3)) \n",
    "print(\"Number of rows excluded due to missing key parameters:\", num_rows_excluded3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining rows after removing samples with missing key parameters: 14943\n",
      "Number of rows excluded due to missing key parameters: 1763\n"
     ]
    }
   ],
   "source": [
    "mask = (\n",
    "    (filtered_data2['well_depth_ft'].notna() | filtered_data2['well_depth2_ft'].notna()) &\n",
    "    (filtered_data2['Temp_C'].notna() | filtered_data2['model_temp_c'].notna()) &\n",
    "    (filtered_data2['ph'].notna() | filtered_data2['model_pH'].notna()) &\n",
    "    filtered_data2['Alk_mgL'].notna() &\n",
    "    filtered_data2['Si_mgL'].notna()\n",
    ")\n",
    " \n",
    "filtered_data3 = filtered_data2[mask]\n",
    "num_rows_filtered2 = len(filtered_data2)\n",
    "num_rows_filtered3 = len(filtered_data3)\n",
    "num_rows_excluded3 = num_rows_filtered2 - num_rows_filtered3\n",
    "\n",
    "print(\"Remaining rows after removing samples with missing key parameters:\", num_rows_filtered3)\n",
    "print(\"Number of rows excluded due to missing key parameters:\", num_rows_excluded3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After charge balance filter: 16706\n",
      "After missing-key-parameter filter: 14943 -> excluded: 1763\n",
      "        well_depth_ft  well_depth2_ft  Temp_C    ph  Alk_mgL  Si_mgL  \\\n",
      "1               744.0             NaN   27.98  7.70     98.0     NaN   \n",
      "3               625.0             NaN   23.65  7.47    260.0     NaN   \n",
      "9               904.0             NaN   22.09  7.82    190.0     NaN   \n",
      "11             1000.0             NaN   27.80  8.04    210.0     NaN   \n",
      "12             1200.0             NaN   20.40  7.35    250.0     NaN   \n",
      "...               ...             ...     ...   ...      ...     ...   \n",
      "122387          320.0             NaN    7.00  7.80    861.0     NaN   \n",
      "122423            NaN             NaN    9.00  7.40    670.0    19.0   \n",
      "122834            NaN             NaN    6.00  4.70      3.0    32.0   \n",
      "122843          258.0             NaN    8.50  9.10    275.0     NaN   \n",
      "122865          275.0             NaN    8.00  7.40    407.0     NaN   \n",
      "\n",
      "       Alk_mgL_flag Si_mgL_flag  \n",
      "1               NaN         NaN  \n",
      "3               NaN         NaN  \n",
      "9               NaN         NaN  \n",
      "11              NaN         NaN  \n",
      "12              NaN         NaN  \n",
      "...             ...         ...  \n",
      "122387          NaN         NaN  \n",
      "122423          NaN         NaN  \n",
      "122834          NaN         NaN  \n",
      "122843          NaN         NaN  \n",
      "122865          NaN         NaN  \n",
      "\n",
      "[1763 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "step2=filtered_data2\n",
    "has_well_depth = step2['well_depth2_ft'].notna() | step2['well_depth_ft'].notna()\n",
    "\n",
    "# Temp/pH/Alk/Silica: require measured values (modelling fields not used)\n",
    "has_temp = step2['Temp_C'].notna()\n",
    "has_pH = step2['ph'].notna()\n",
    "has_alk = step2['Alk_mgL'].notna()\n",
    "has_silica = step2['Si_mgL'].notna()\n",
    "\n",
    "mask = has_well_depth & has_temp & has_pH & has_alk & has_silica\n",
    "step3 = step2[mask]\n",
    "\n",
    "print(\"After charge balance filter:\", len(step2))\n",
    "print(\"After missing-key-parameter filter:\", len(step3),\n",
    "      \"-> excluded:\", len(step2) - len(step3))\n",
    "\n",
    "diff = step2.loc[~mask]  # These are the excluded rows\n",
    "print(diff[['well_depth_ft','well_depth2_ft',\n",
    "            'Temp_C','ph','Alk_mgL','Si_mgL',\n",
    "            'Alk_mgL_flag','Si_mgL_flag']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining rows after removing samples missing key parameters (USGS style): 15700\n",
      "Number of rows excluded due to missing key parameters (USGS style): 1006\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def is_valid_measurement(value, flag=None):\n",
    "    \"\"\"\n",
    "    Determines if a measurement is valid (not missing), considering flags.\n",
    "    Flags like '<' or 'ppm' mean censored but valid.\n",
    "    \"\"\"\n",
    "    if pd.notna(value):\n",
    "        return True\n",
    "    else:\n",
    "        # If value missing (NaN), count as invalid\n",
    "        return False\n",
    "    \n",
    "# Well depth: either sample level or site level present\n",
    "has_well_depth = filtered_data2['well_depth2_ft'].notna() | filtered_data2['well_depth_ft'].notna()\n",
    "\n",
    "# Temperature, pH: measured values present (ignore modeled)\n",
    "has_temp = filtered_data2['Temp_C'].notna()\n",
    "has_pH = filtered_data2['ph'].notna()\n",
    "\n",
    "# Alkalinity and Silica: treat flagged values as presence\n",
    "# For Alk_mgL, count as present if measurement is present OR flagged as '<' or 'ppm'\n",
    "\n",
    "alk_present = (\n",
    "    filtered_data2['Alk_mgL'].notna() |\n",
    "    filtered_data2['Alk_mgL_flag'].isin(['<', '< ppm', 'ppm', np.nan])\n",
    ")\n",
    "# The np.nan inclusion means if flag missing, count as present since no negative qualifier\n",
    "\n",
    "silica_present = (\n",
    "    filtered_data2['Si_mgL'].notna() |\n",
    "    filtered_data2['Si_mgL_flag'].isin(['<', '< ppm', 'ppm', np.nan])\n",
    ")\n",
    "# Combine all conditions for filtering\n",
    "mask = has_well_depth & has_temp & has_pH & alk_present & silica_present\n",
    "\n",
    "filtered_data3 = filtered_data2[mask]\n",
    "\n",
    "num_rows_filtered3 = len(filtered_data3)\n",
    "num_rows_excluded3 = len(filtered_data2) - num_rows_filtered3\n",
    "\n",
    "print(\"Remaining rows after removing samples missing key parameters (USGS style):\", num_rows_filtered3)\n",
    "print(\"Number of rows excluded due to missing key parameters (USGS style):\", num_rows_excluded3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alkalinity flag values:\n",
      "Alk_mgL_flag\n",
      "NaN    16687\n",
      "<         19\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Silica flag values:\n",
      "Si_mgL_flag\n",
      "NaN    14199\n",
      "ppm     2484\n",
      "<         23\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# See unique values in Alk_mgL_flag, with counts\n",
    "print(\"Alkalinity flag values:\")\n",
    "print(filtered_data2['Alk_mgL_flag'].value_counts(dropna=False))\n",
    "\n",
    "print(\"\\nSilica flag values:\")\n",
    "print(filtered_data2['Si_mgL_flag'].value_counts(dropna=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining rows after removing samples with missing key parameters: 14943\n",
      "Number of rows excluded due to missing key parameters: 1763\n",
      "count    14943.000000\n",
      "mean        18.940450\n",
      "std         18.153428\n",
      "min          0.000000\n",
      "25%          9.000000\n",
      "50%         15.000000\n",
      "75%         25.000000\n",
      "max       1000.000000\n",
      "Name: Si_mgL, dtype: float64\n",
      "62822     1000.0\n",
      "2617       843.0\n",
      "104993     400.0\n",
      "115836     290.0\n",
      "108164     244.0\n",
      "41193      240.0\n",
      "33255      187.0\n",
      "66426      180.0\n",
      "641        178.0\n",
      "66427      170.0\n",
      "Name: Si_mgL, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "mask = (\n",
    "    (filtered_data2['well_depth_ft'].notna() | filtered_data2['well_depth2_ft'].notna()) &\n",
    "    (filtered_data2['Temp_C'].notna() | filtered_data2['model_temp_c'].notna()) &\n",
    "    (filtered_data2['ph'].notna() | filtered_data2['model_pH'].notna()) &\n",
    "    filtered_data2['Alk_mgL'].notna() &\n",
    "    filtered_data2['Si_mgL'].notna()\n",
    ")\n",
    " \n",
    "filtered_data3 = filtered_data2[mask]\n",
    "num_rows_filtered2 = len(filtered_data2)\n",
    "num_rows_filtered3 = len(filtered_data3)\n",
    "num_rows_excluded3 = num_rows_filtered2 - num_rows_filtered3\n",
    "\n",
    "print(\"Remaining rows after removing samples with missing key parameters:\", num_rows_filtered3)\n",
    "print(\"Number of rows excluded due to missing key parameters:\", num_rows_excluded3)\n",
    "\n",
    "print(filtered_data3['Si_mgL'].describe())\n",
    "print(filtered_data3.sort_values(by='Si_mgL', ascending=False)['Si_mgL'].head(10))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Excluded rows with anomalously high silica values.\n",
      "Remaining rows after exclusion: 14941\n"
     ]
    }
   ],
   "source": [
    "# Get the indices of the two rows with highest silica (Si_mgL)\n",
    "top_two_indices = filtered_data3.sort_values(by='Si_mgL', ascending=False).head(2).index\n",
    "\n",
    "# Drop those two rows from DataFrame\n",
    "filtered_data_final = filtered_data3.drop(top_two_indices)\n",
    "\n",
    "print(\"Excluded rows with anomalously high silica values.\")\n",
    "print(f\"Remaining rows after exclusion: {len(filtered_data_final)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining rows after keeping only ['FL', 'CA', 'TX', 'AZ', 'NM']: 4810\n",
      "Filtered data for California: 1268\n",
      "Filtered data for Florida: 386\n",
      "Filtered data for Texas: 2388\n",
      "Filtered data for Arizona: 605\n",
      "Filtered data for New Mexico: 163\n"
     ]
    }
   ],
   "source": [
    "# List of state abbreviations you want to keep\n",
    "states_to_keep = [\"FL\", \"CA\", \"TX\", \"AZ\", \"NM\"]\n",
    "\n",
    "# Filter DataFrame to only keep those states\n",
    "data_by_state = filtered_data_final[filtered_data_final['state_alpha'].isin(states_to_keep)]\n",
    "\n",
    "print(f\"Remaining rows after keeping only {states_to_keep}: {len(data_by_state)}\")\n",
    "data_CA = data_by_state[data_by_state['state_alpha'] == 'CA']\n",
    "print(f\"Filtered data for California: {len(data_CA)}\")\n",
    "data_FL = data_by_state[data_by_state['state_alpha'] == 'FL']\n",
    "print(f\"Filtered data for Florida: {len(data_FL)}\")\n",
    "data_TX = data_by_state[data_by_state['state_alpha'] == 'TX']\n",
    "print(f\"Filtered data for Texas: {len(data_TX)}\")\n",
    "data_AZ = data_by_state[data_by_state['state_alpha'] == 'AZ']\n",
    "print(f\"Filtered data for Arizona: {len(data_AZ)}\")\n",
    "data_NM = data_by_state[data_by_state['state_alpha'] == 'NM']\n",
    "print(f\"Filtered data for New Mexico: {len(data_NM)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features used for clustering: ['Ca_mgL_frac', 'Mg_mgL_frac', 'Na_mgL_frac', 'K_mgL_frac', 'Cl_mgL_frac', 'SO4_mgL_frac', 'HCO3_mgl_frac', 'Si_molL_frac', 'TDS_log10', 'ph', 'Temp_C']\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input X contains NaN.\nKMeans does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 85\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m K_range:\n\u001b[1;32m     84\u001b[0m     km \u001b[38;5;241m=\u001b[39m KMeans(n_clusters\u001b[38;5;241m=\u001b[39mk, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m, n_init\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m---> 85\u001b[0m     \u001b[43mkm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_scaled\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     86\u001b[0m     between_ss \u001b[38;5;241m=\u001b[39m tot_ss \u001b[38;5;241m-\u001b[39m km\u001b[38;5;241m.\u001b[39minertia_\n\u001b[1;32m     87\u001b[0m     r2_values\u001b[38;5;241m.\u001b[39mappend(between_ss \u001b[38;5;241m/\u001b[39m tot_ss)\n",
      "File \u001b[0;32m~/miniconda3/envs/watertap-flex/lib/python3.11/site-packages/sklearn/base.py:1365\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1358\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1360\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1361\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1362\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1363\u001b[0m     )\n\u001b[1;32m   1364\u001b[0m ):\n\u001b[0;32m-> 1365\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/watertap-flex/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1454\u001b[0m, in \u001b[0;36mKMeans.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1426\u001b[0m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   1427\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   1428\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute k-means clustering.\u001b[39;00m\n\u001b[1;32m   1429\u001b[0m \n\u001b[1;32m   1430\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1452\u001b[0m \u001b[38;5;124;03m        Fitted estimator.\u001b[39;00m\n\u001b[1;32m   1453\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1454\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1455\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1456\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1457\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1458\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat64\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1459\u001b[0m \u001b[43m        \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1460\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy_x\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1461\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1462\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1464\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_params_vs_input(X)\n\u001b[1;32m   1466\u001b[0m     random_state \u001b[38;5;241m=\u001b[39m check_random_state(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrandom_state)\n",
      "File \u001b[0;32m~/miniconda3/envs/watertap-flex/lib/python3.11/site-packages/sklearn/utils/validation.py:2954\u001b[0m, in \u001b[0;36mvalidate_data\u001b[0;34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[0m\n\u001b[1;32m   2952\u001b[0m         out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[1;32m   2953\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[0;32m-> 2954\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2955\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[1;32m   2956\u001b[0m     out \u001b[38;5;241m=\u001b[39m _check_y(y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n",
      "File \u001b[0;32m~/miniconda3/envs/watertap-flex/lib/python3.11/site-packages/sklearn/utils/validation.py:1105\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m   1099\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1100\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m{\u001b[39;00marray\u001b[38;5;241m.\u001b[39mndim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m while dim <= 2 is required\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcontext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1102\u001b[0m     )\n\u001b[1;32m   1104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_all_finite:\n\u001b[0;32m-> 1105\u001b[0m     \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1106\u001b[0m \u001b[43m        \u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1107\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1108\u001b[0m \u001b[43m        \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1109\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1110\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1112\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copy:\n\u001b[1;32m   1113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_numpy_namespace(xp):\n\u001b[1;32m   1114\u001b[0m         \u001b[38;5;66;03m# only make a copy if `array` and `array_orig` may share memory`\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/watertap-flex/lib/python3.11/site-packages/sklearn/utils/validation.py:120\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 120\u001b[0m \u001b[43m_assert_all_finite_element_wise\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmsg_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmsg_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/watertap-flex/lib/python3.11/site-packages/sklearn/utils/validation.py:169\u001b[0m, in \u001b[0;36m_assert_all_finite_element_wise\u001b[0;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[1;32m    153\u001b[0m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[1;32m    155\u001b[0m     msg_err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    156\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept missing values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    157\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#estimators-that-handle-nan-values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    168\u001b[0m     )\n\u001b[0;32m--> 169\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[0;31mValueError\u001b[0m: Input X contains NaN.\nKMeans does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = data_by_state.copy()\n",
    "\n",
    "# === Stage 1: Ion & TDS Processing ===\n",
    "# Define which columns are cations, anions, etc.\n",
    "cations = ['Ca_mgL', 'Mg_mgL', 'Na_mgL', 'K_mgL']  # replace with your actual columns\n",
    "anions = ['Cl_mgL', 'SO4_mgL', 'HCO3_mgl']         # replace with your actual columns\n",
    "silica_col = 'Si_mgL'\n",
    "dissolved_solids_col = 'TDS_mgL'\n",
    "\n",
    "# Molar masses (g/mol) for conversions\n",
    "molar_masses = {\n",
    "    'Ca_mgL': 40.078,\n",
    "    'Mg_mgL': 24.305,\n",
    "    'Na_mgL': 22.990,\n",
    "    'K_mgL': 39.098,\n",
    "    'Cl_mgL': 35.45,\n",
    "    'SO4_mgL': 96.06,  # SO4-2\n",
    "    'HCO3_mgl': 61.0168,\n",
    "    silica_col: 60.08  # SiO2\n",
    "}\n",
    "\n",
    "# Charges for equivalent calculations\n",
    "charges = {\n",
    "    'Ca_mgL': 2,\n",
    "    'Mg_mgL': 2,\n",
    "    'Na_mgL': 1,\n",
    "    'K_mgL': 1,\n",
    "    'Cl_mgL': 1,\n",
    "    'SO4_mgL': 2,\n",
    "    'HCO3_mgl': 1\n",
    "}\n",
    "\n",
    "# Convert mg/L to equivalents for major ions\n",
    "for ion in cations + anions:\n",
    "    eq_col = ion + '_eq'\n",
    "    df[eq_col] = (df[ion] / molar_masses[ion]) * charges[ion]\n",
    "\n",
    "# Totals\n",
    "df['total_cations_eq'] = df[[c + '_eq' for c in cations]].sum(axis=1)\n",
    "df['total_anions_eq']  = df[[a + '_eq' for a in anions]].sum(axis=1)\n",
    "\n",
    "# Fractions\n",
    "for ion in cations:\n",
    "    df[ion + '_frac'] = df[ion + '_eq'] / df['total_cations_eq']\n",
    "for ion in anions:\n",
    "    df[ion + '_frac'] = df[ion + '_eq'] / df['total_anions_eq']\n",
    "\n",
    "# Print just the new columns created\n",
    "#new_cols = [ion + '_eq' for ion in cations + anions] + ['total_cations_eq', 'total_anions_eq']\n",
    "#print(df[new_cols].head())\n",
    "\n",
    "# Silica fraction\n",
    "df['Si_molL'] = df[silica_col] / molar_masses[silica_col]\n",
    "df['total_ion_molL'] = (\n",
    "    df[[ion + '_eq' for ion in cations + anions]]\n",
    "    .div([charges[i] for i in cations + anions], axis=1)\n",
    "    .sum(axis=1)\n",
    ")\n",
    "df['Si_molL_frac'] = df['Si_molL'] / df['total_ion_molL']\n",
    "\n",
    "# Log transform TDS\n",
    "df['TDS_log10'] = np.log10(df[dissolved_solids_col])\n",
    "\n",
    "# === Stage 2: Data Normalization ===\n",
    "# Prepare dataframe for clustering\n",
    "untouched = ['ph', 'Temp_C']\n",
    "features = [col for col in df.columns if col.endswith('_frac')] + ['TDS_log10'] + untouched\n",
    "X = df[features]\n",
    "print(\"Features used for clustering:\", X.columns.tolist())\n",
    "# Scale\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# R² vs k\n",
    "K_range = range(1, 10)\n",
    "r2_values = []\n",
    "tot_ss = np.sum((X_scaled - X_scaled.mean(axis=0)) ** 2)\n",
    "\n",
    "for k in K_range:\n",
    "    km = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    km.fit(X_scaled)\n",
    "    between_ss = tot_ss - km.inertia_\n",
    "    r2_values.append(between_ss / tot_ss)\n",
    "\n",
    "plt.plot(K_range, r2_values, marker='o')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('R²')\n",
    "plt.title('Elbow method for k-means')\n",
    "plt.show()\n",
    "\n",
    "# Final k=4 (example)\n",
    "k_optimal = 4\n",
    "km_final = KMeans(n_clusters=k_optimal, random_state=42, n_init=10)\n",
    "df['Cluster'] = km_final.fit_predict(X_scaled)\n",
    "\n",
    "# Save results\n",
    "#df.to_csv(\"geochem_clusters_with_metadata.csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "watertap-flex",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
